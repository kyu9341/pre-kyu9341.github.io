---
layout: post
title: "ICTë©˜í† ë§ ë”¥ëŸ¬ë‹ êµìœ¡ 2ì¼ì°¨"
subtitle: "Machine Learning"
date: 2019-11-24 10:01:25
author: kwon
categories: MachineLearning
---
ì´ë²ˆì—ëŠ” ë‘˜ì§¸ ë‚  ë°°ìš´ ë‚´ìš©ì„ ë¦¬ë·°í•´ë³´ê² ë‹¤. ë‘˜ì§¸ ë‚ ì—ëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ ì´ë¡ , Azure Cloud GPUí™œìš©, Cifar10 ì´ë¯¸ì§€ ë¶„ë¥˜ì— ëŒ€í•´ì„œ ë°°ì› ë‹¤.

### Convolutional Neural Network(CNN)
CNN(í•©ì„±ê³± ì‹ ê²½ë§)ì€ í•„í„°ë§ ê¸°ë²•ì„ ì¸ê³µì‹ ê²½ë§ì— ì ìš©í•¨ìœ¼ë¡œì¨ ì´ë¯¸ì§€ë¥¼ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤. ì´ë¯¸ì§€ë¥¼ Dense(fully connected) Layerë¡œ ì²˜ë¦¬í•˜ë ¤ í•œë‹¤ë©´ featureê°€ ë„ˆë¬´ ë§ì•„ì ¸ ë¶ˆí•„ìš”í•œ weightê°€ ë§ì•„ì§€ê³  íš¨ìœ¨ì´ ë–¨ì–´ì§€ê²Œ ëœë‹¤. ë˜í•œ Dense LayerëŠ” 1ì°¨ì› ë°ì´í„°ë§Œ inputìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 3ì°¨ì› ë°ì´í„°ë¥¼ í‰íƒ„í™”í•˜ì—¬ ì…ë ¥í•´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ 3ì°¨ì› ë°ì´í„°ì˜ ê³µê°„ì  ì •ë³´ê°€ ì†Œì‹¤ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ë°˜ë©´ Convolutional Layerì—ì„œëŠ” í˜•ìƒì„ ìœ ì§€í•œë‹¤. ì…/ì¶œë ¥ ëª¨ë‘ 3ì°¨ì› ë°ì´í„°ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— ê³µê°„ì  ì •ë³´ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.

<div style="width: 100%; height: 400px;">
    <img src="https://kyu9341.github.io/assets/cnn.png" style="width: 100%
    ; height: 400px;">
</div>

#### Convolutional Layer
Convolutionì€ í•©ì„±ê³±ì´ë¼ëŠ” ëœ»ì´ë‹¤.

ì´ë¯¸ì§€ ë°ì´í„°ëŠ” ë†’ì´Xë„ˆë¹„Xì±„ë„ì˜ 3ì°¨ì› í…ì„œ (tensor)ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. ë§Œì•½, ì´ë¯¸ì§€ì˜ ìƒ‰ìƒì´ RGB ì½”ë“œë¡œ í‘œí˜„ë˜ì—ˆë‹¤ë©´, ì±„ë„ì˜ í¬ê¸°ëŠ” 3ì´ ë˜ë©° ê°ê°ì˜ ì±„ë„ì—ëŠ” R, G, B ê°’ì´ ì €ì¥ëœë‹¤.

#### padding
1ë§Œí¼ íŒ¨ë”©í–ˆë‹¤ê³  í•œë‹¤ë©´, ì…ë ¥ ë°ì´í„° ì‚¬ë°© 1í”½ì…€ì„ íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ì›Œ ëŠ˜ë¦¬ëŠ” ê²ƒì„ ë§í•œë‹¤. ì£¼ë¡œ ì¶œë ¥ í¬ê¸°ë¥¼ ì¡°ì •í•  ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”ë° ì´ë¥¼í…Œë©´ (5, 5)ë°ì´í„°ì— (3, 3)í•„í„°ë¥¼ ì ìš©í•˜ë©´ ì¶œë ¥ì´ (3, 3)ì´ ëœë‹¤. ì´ì²˜ëŸ¼ í•©ì„±ê³± ì—°ì‚°ì„ ê±°ì¹  ë•Œ ë§ˆë‹¤ í¬ê¸°ê°€ ì‘ì•„ì§€ê²Œ ë˜ëŠ”ë° ì¶œë ¥ í¬ê¸°ê°€ ë„ˆë¬´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì‚¬ìš©í•œë‹¤. íŒ¨ë”©ì„ ì‚¬ìš©í•˜ë©´ ê·¸ë§Œí¼ ì…ë ¥ ë°ì´í„°ë¥¼ ê·¸ê²Œ ë§Œë“¤ì–´ ì¶œë ¥ ë°ì´í„°ë¥¼ ì…ë ¥ ë°ì´í„°ì™€ ë™ì¼í•œ í˜•ìƒìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì…ë ¥ ë°ì´í„°ì˜ ê³µê°„ì  í¬ê¸°ë¥¼ ê³ ì •í•œ ì±„ë¡œ ë‹¤ìŒ ê³„ì¸µì— ì „ë‹¬í•  ìˆ˜ ìˆë‹¤.



#### Pooling layer
í’€ë§ì€ ê°€ë¡œ/ì„¸ë¡œ ë°©í–¥ì˜ ê³µê°„ì„ ì¤„ì´ëŠ” ì—°ì‚°ì´ë‹¤. í’€ë§ì˜ ì¢…ë¥˜ë¡œëŠ” Max Poolingê³¼ Average Poolingì´ ìˆëŠ”ë° ì´ë¯¸ì§€ ì¸ì‹ ë¶„ì•¼ì—ì„œëŠ” ì£¼ë¡œ Max Poolingì„ ì‚¬ìš©í•œë‹¤.

<div style="width: 100%; height: 400px;">
    <img src="https://kyu9341.github.io/assets/pooling.png" style="width: 100%
    ; height: 400px;">
</div>

- Average pooling : ì§€ì •ëœ êµ¬ì—­ì˜ í‰ê· ê°’ì„ ê°€ì ¸ì™€ ëŒ€ì²´í•¨

- Max pooling layer: ì§€ì •ëœ êµ¬ì—­ì˜ ê°€ì¥ í° ê°’ì„ ê°€ì ¸ì™€ ëŒ€ì²´í•¨(ì‚¬ì†Œí•œ ì •ë³´ ì°¨ì´ëŠ” ë¬´ì‹œ)

  ïƒ¨	ì´ë¯¸ì§€ê°€ ê°€ì§€ëŠ” í° íŠ¹ì§•ì„ ìœ ì§€í•˜ë©° ì‚¬ì´ì¦ˆë¥¼ ì¤„ì„ (íŠ¹ì§• ì¼ë°˜í™” ê°€ëŠ¥)

  ïƒ¨	ì˜ˆë¥¼ ë“¤ì–´ ê°•ì•„ì§€ì˜ ì¢…ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë„ í° íŠ¹ì§•ë§Œ ê°€ì§€ê³  ì»¤ë²„ê°€ëŠ¥

  ïƒ¨	ë°ì´í„°ê°€ ì§€ì—­ì ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•Šë„ë¡ í•¨

## ì‹¤ìŠµ1

ì²«ë²ˆì§¸ëŠ” ê°„ë‹¨í•˜ê²Œ ì´ë¯¸ì§€ë§Œ ë¶ˆëŸ¬ì™€ì„œ í™•ì¸í•˜ëŠ” ê³¼ì •ì´ë‹¤.

#### 2019.11.24. ë”¥-ëŸ¬ë‹ ê³¼ì • CNN

### ì¤€ë¹„ ì‹¤ìŠµ. Image ì‚´í´ë³´ê¸°


```python
# 1. package ê°€ì ¸ì˜¤ê¸°

from PIL import Image

```


```python
# 2. ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°

image_gray = Image.open('01image.png')
image_color = Image.open('02image.png')
image_jet = Image.open('jet.png')


print(image_gray.size)
print(image_gray)
# print(image_jet)


print(image_color.size) # sizeë§Œ ë½‘ìœ¼ë©´ ê°€ë¡œ ì„¸ë¡œë§Œ ë‚˜ì˜´
print(image_gray.size)
# print(image_jet.size)
```

    (28, 28)
    <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x1405C55AE08>
    (32, 32)
    (28, 28)



```python
# 3. ì´ë¯¸ì§€ arrayë¡œ ë³€í™˜í•˜ê¸°

import numpy as np

image_array = np.array(image_gray)
image_array_color = np.array(image_color)
# image_array_jet = np.array(image_jet)


print(image_array.shape) # í•™ìŠµ ì‹œí‚¬ë•Œ í•­ìƒ numpy dataë¡œ ë³€í™˜í•˜ë©° shapeë¥¼ í™•ì¸í•´ì•¼í•¨
print(image_array_color.shape) # numpy arrayë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥í•˜ë©´ (32, 32, 3)ì²˜ëŸ¼ ëìŠ¤ë„ ì¶œë ¥ ê°€ëŠ¥
# 3ì°¨ì› ë°ì´í„°ì—¬ì•¼ë§Œ CNNì— ë„£ì–´ì„œ í•™ìŠµì´ ê°€ëŠ¥ ë”°ë¼ì„œ gray scaleì€ reshapeí•´ì¤˜ì•¼í•¨
# print(image_array_jet.shape)
reshpaed_1d = image_array.reshape(28*28) # gray image ->reshape
reshaped_3d = image_array.reshape(28, 28, 1) # gray image -> 3dë¡œ ë³€ê²½í•˜ì—¬ cnnì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨

print(reshpaed_1d.shape)
print(reshaped_3d.shape)
```

    (28, 28)
    (32, 32, 3)
    (784,)
    (28, 28, 1)



```python
# 4. ì´ë¯¸ì§€ ì‚´í´ë³´ê¸°

# print(image_array)
print(image_array_color)
# print(image_array_jet)
# print(reshaped_3d)

```

    [[[125 125 116]
      [110 101  91]
      [102  90  83]
      ...
      [202 207 214]
      [200 205 212]
      [202 208 214]]
      ...
      [143 117  82]
      [143 116  84]
      [144 116  86]]]



```python
# 5. ì´ë¯¸ì§€ ì‹œê°í™”
import matplotlib.pyplot as plt

plt.imshow(image_array, cmap=plt.cm.gray) # gray scaleë¥¼ ì •ìƒì ìœ¼ë¡œ ë„ìš°ë ¤ë©´ color map ì„ ì§€ì •í•´ì•¼í•¨
plt.imshow(image_array_color)
# plt.imshow(image_array_jet)
```

    <matplotlib.image.AxesImage at 0x1405c59de48>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_6_1.png" style="width: 50%
    ; height: 300px;">
</div>


## ì‹¤ìŠµ2
ë‘ë²ˆì§¸ ì‹¤ìŠµì€ CNNëª¨ë¸ì„ ì ìš©í•˜ê¸° ì „ì— MLTë§Œ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í•™ìŠµì‹œì¼œë³´ì•˜ë‹¤. MLTëŠ” 1ì°¨ì› ë°ì´í„°ë§Œ inputìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— gray scaleì˜ 2ì°¨ì› ë°ì´í„°ëŠ” 1ì°¨ì›ìœ¼ë¡œ reshapeì„ í•´ì£¼ì–´ì•¼ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.






#### 2019.11.24. ë”¥-ëŸ¬ë‹ ê³¼ì • CNN

### ì²«ë²ˆì§¸ ì‹¤ìŠµ. Keras ëª¨ë¸ ìƒì„±/í•™ìŠµ - MNIST : MLP
[Keras Dataset](https://keras.io/ko/datasets/#mnist)


```python
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

from keras.datasets import mnist  # ë§ì´ ì“°ì´ëŠ” ë°ì´í„° ì¼€ë¼ìŠ¤ì—ì„œ ì œê³µ

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
```

    (60000, 28, 28)
    (60000,)
    (10000, 28, 28)
    (10000,)



```python
# 2. ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸í•˜ê¸° ğŸ–¼
import matplotlib.pyplot as plt

image = X_train[1]

plt.imshow(image, cmap=plt.cm.gray)
```

    <matplotlib.image.AxesImage at 0x12a2b101c48>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_3_1.png" style="width: 50%
    ; height: 300px;">
</div>


```python
print(X_train.shape) # 28*28 ì´ë¯¸ì§€ (2ì°¨ì›) 6ë§Œê°œ -> 1ì°¨ì› 6ë§Œê°œ
```

    (60000, 28, 28)


```python
# 3-1. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : 2ì°¨ì›->1ì°¨ì› ğŸŒŸğŸŒŸğŸŒŸ

X_train = X_train.reshape((60000, 28 * 28)) # -> 1ì°¨ì› 6ë§Œê°œë¡œ ë³€í™˜
X_test = X_test.reshape((10000, 28 * 28))

print(X_train.shape)
print(X_test.shape)

print(X_train)
```

    (60000, 784)
    (10000, 784)
    [[0 0 0 ... 0 0 0]
     [0 0 0 ... 0 0 0]
     [0 0 0 ... 0 0 0]
     ...
     [0 0 0 ... 0 0 0]
     [0 0 0 ... 0 0 0]
     [0 0 0 ... 0 0 0]]


```python
# 3-2. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : Normalzation
X_train = X_train / 255.0
X_test = X_test / 255.0

print(X_train[9])
```



```python
# 4. Label ì „ì²˜ë¦¬ (one-hot encoding)
print(y_train[:10]) # ì•ì—ì„œ 10ê°œ, label : ìˆ«ìê°€ ì–´ë–¤ ìˆ«ìì¸ì§€
```

    [5 0 4 1 9 2 1 3 1 4]



```python
# 4. Label ì „ì²˜ë¦¬ (one-hot encoding)

from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

print(y_train[:10])
```

    [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]



```python
# 4-1 Validation ì…‹ ë‚˜ëˆ„ê¸°

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,
                                                    test_size=0.2,
                                                    random_state=9)

print(X_train.shape)
print(y_train.shape)

print(X_val.shape)
print(y_val.shape)


```

    (48000, 784)
    (48000, 10)
    (12000, 784)
    (12000, 10)



```python
# 5. ëª¨ë¸ ìƒì„± : MLP
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(512, input_dim=784, activation='relu')) # ë‹¤ìŒ ë…¸ë“œì˜ ìˆ˜ : 512(inputdataê°€ 784ì´ë¯€ë¡œ ì ë‹¹íˆ í¬ê²Œ)
model.add(Dense(10, activation='softmax')) # ì¶œë ¥ ì¸µ 10ê°œì¤‘ ì–´ë–¤ê±´ì§€ ì•Œê¸° ìœ„í•´ 10ìœ¼ë¡œ ì§€ì •

print(model.summary())
```

    Model: "sequential_6"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_9 (Dense)              (None, 512)               401920    
    _________________________________________________________________
    dense_10 (Dense)             (None, 10)                5130      
    =================================================================
    Total params: 407,050
    Trainable params: 407,050
    Non-trainable params: 0
    _________________________________________________________________
    None



```python
# 6. Compile - Optimizer, Loss function ì„¤ì •
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
```


```python
# 7. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°

batch_size=128
epochs=10

history = model.fit(X_train, y_train,
         epochs=epochs,
         batch_size=batch_size,
         validation_data=(X_val, y_val), # validation_set ì ìš© (ê¼­ ê°™ì´ í•´ì£¼ëŠ”ê²Œ ì¢‹ìŒ)
         verbose=1)
```

    Train on 48000 samples, validate on 12000 samples
    Epoch 1/10
    48000/48000 [==============================] - 2s 37us/step - loss: 1.2137 - accuracy: 0.7386 - val_loss: 0.6995 - val_accuracy: 0.8526
    Epoch 2/10
    48000/48000 [==============================] - 2s 34us/step - loss: 0.5725 - accuracy: 0.8683 - val_loss: 0.4960 - val_accuracy: 0.8789
    Epoch 3/10
    48000/48000 [==============================] - 2s 42us/step - loss: 0.4501 - accuracy: 0.8855 - val_loss: 0.4243 - val_accuracy: 0.8904
    Epoch 4/10
    48000/48000 [==============================] - 2s 40us/step - loss: 0.3967 - accuracy: 0.8948 - val_loss: 0.3847 - val_accuracy: 0.8984
    Epoch 5/10
    48000/48000 [==============================] - 2s 40us/step - loss: 0.3648 - accuracy: 0.9019 - val_loss: 0.3599 - val_accuracy: 0.9035
    Epoch 6/10
    48000/48000 [==============================] - 2s 40us/step - loss: 0.3425 - accuracy: 0.9064 - val_loss: 0.3419 - val_accuracy: 0.9077
    Epoch 7/10
    48000/48000 [==============================] - 2s 38us/step - loss: 0.3257 - accuracy: 0.9103 - val_loss: 0.3276 - val_accuracy: 0.9109
    Epoch 8/10
    48000/48000 [==============================] - 2s 40us/step - loss: 0.3121 - accuracy: 0.9145 - val_loss: 0.3155 - val_accuracy: 0.9133
    Epoch 9/10
    48000/48000 [==============================] - 2s 41us/step - loss: 0.3006 - accuracy: 0.9174 - val_loss: 0.3064 - val_accuracy: 0.9160
    Epoch 10/10
    48000/48000 [==============================] - 2s 40us/step - loss: 0.2906 - accuracy: 0.9201 - val_loss: 0.2977 - val_accuracy: 0.9183



```python
# 8. ëª¨ë¸ í‰ê°€í•˜ê¸°
test_loss, test_acc = model.evaluate(X_test, y_test)

print(test_loss, test_acc)
```

    10000/10000 [==============================] - 0s 25us/step
    0.2771936253398657 0.9223999977111816



```python
# 9. ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•´ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ğŸ–¼

import numpy
for index in numpy.random.choice(len(y_test), 3, replace = False):
    predicted = model.predict(X_test[index:index + 1])[0]
    label = y_test[index]
    result_label = numpy.where(label == numpy.amax(label))
    result_predicted = numpy.where(predicted == numpy.amax(predicted))
    title = "Label value = %s  Predicted value = %s " % (result_label[0], result_predicted[0])

    fig = plt.figure(1, figsize = (3,3))
    ax1 = fig.add_axes((0,0,.8,.8))
    ax1.set_title(title)
    images = X_test
    plt.imshow(images[index].reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')
    plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_14_0.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_14_1.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_14_2.png" style="width: 50%
    ; height: 300px;">
</div>


```python
# 10. í•™ìŠµ ì‹œê°í™”í•˜ê¸°
import matplotlib.pyplot as plt

plt.plot(history.history['val_accuracy'])
plt.plot(history.history['accuracy'])
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



# plt.plot(history.history['val_accuracy'])

```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_15_0.png" style="width: 50%
    ; height: 300px;">
</div>



## ì‹¤ìŠµ3
ì´ë²ˆì—ëŠ” ì‹¤ì œë¡œ CNNì„ ì‚¬ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ì„ ìˆ˜í–‰í–ˆë‹¤. MLTë¥¼ ìˆ˜í–‰í•˜ê¸° ì´ì „ì— Convolution Layer ë° Pooling Layerë¥¼ ê±°ì³ í•™ìŠµì´ ì§„í–‰ëœë‹¤.

#### 2019.11.24. ë”¥-ëŸ¬ë‹ ê³¼ì • CNN

### ë‘ë²ˆì§¸ ì‹¤ìŠµ. Keras ëª¨ë¸ ìƒì„±/í•™ìŠµ - MNIST : CNN
[Keras Dataset](https://keras.io/ko/datasets/#mnist)


```python
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
from keras.datasets import mnist
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,
                                                 test_size=0.2,
                                                 random_state=9)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
```

    (48000, 28, 28)
    (48000,)
    (10000, 28, 28)
    (10000,)



```python
# 2. ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸í•˜ê¸° ğŸ–¼
import matplotlib.pyplot as plt

image = X_train[2]

plt.imshow(image, cmap=plt.cm.gray)
```

    <matplotlib.image.AxesImage at 0x240c6b78c48>



<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_3_1a.png" style="width: 50%
    ; height: 300px;">
</div>



```python
# 3-1. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : 2ì°¨ì›->3ì°¨ì› ğŸŒŸğŸŒŸğŸŒŸ

# w=h=28            
# X_train = X_train.reshape(X_train.shape[0], w, h, 1)
# ìœ„ì˜ ë‘ì¤„ë¡œ ëŒ€ì²´ ê°€ëŠ¥ ìœ„ì˜ ë°©ì‹ì´ ì¬ì‚¬ìš©ì— ìš©ì´

X_train = X_train.reshape(48000, 28, 28, 1)
X_val = X_val.reshape(12000, 28,28,1)
X_test = X_test.reshape((10000, 28, 28, 1))

print(X_train.shape)
print(X_val.shape)

print(X_test.shape)

```

    (48000, 28, 28, 1)
    (12000, 28, 28, 1)
    (10000, 28, 28, 1)



```python
# 3-2. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : Normalzation
X_train = X_train / 255.0
X_val = X_val / 255.0
X_test = X_test / 255.0

```


```python
# 4. Label ì „ì²˜ë¦¬ (one-hot encoding)

print(y_train[:10]) # ì•ì—ì„œ 10ê°œ, label : ìˆ«ìê°€ ì–´ë–¤ ìˆ«ìì¸ì§€

from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

print(y_train[:10])

```

    [9 9 9 5 3 2 0 4 2 7]
    [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
     [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]



```python
# 5. ëª¨ë¸ ìƒì„± : CNN ğŸŒŸğŸŒŸğŸŒŸ
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential()
model.add(Conv2D(filters=32,
                kernel_size=(3,3),
                padding='same',
                activation='relu',
                input_shape=(28, 28, 1)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

print(model.summary())

```

    Model: "sequential_10"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_9 (Conv2D)            (None, 28, 28, 32)        320       
    _________________________________________________________________
    max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         
    _________________________________________________________________
    flatten_6 (Flatten)          (None, 6272)              0         
    _________________________________________________________________
    dense_9 (Dense)              (None, 128)               802944    
    _________________________________________________________________
    dense_10 (Dense)             (None, 10)                1290      
    =================================================================
    Total params: 804,554
    Trainable params: 804,554
    Non-trainable params: 0
    _________________________________________________________________
    None



```python
# 6. Compile - Optimizer, Loss function ì„¤ì •
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
```


```python
# 7. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°

batch_size=128
epochs=10

history = model.fit(X_train, y_train,
         epochs=epochs,
         batch_size=batch_size,
         validation_data=(X_val, y_val), # validation_set ì ìš© (ê¼­ ê°™ì´ í•´ì£¼ëŠ”ê²Œ ì¢‹ìŒ)
         verbose=1)
```

    W1124 16:01:22.815376 23724 deprecation_wrapper.py:119] From c:\users\kyu93\appdata\local\programs\python\python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.



    Train on 48000 samples, validate on 12000 samples
    Epoch 1/10
    48000/48000 [==============================] - 10s 203us/step - loss: 1.1062 - accuracy: 0.7491 - val_loss: 0.4400 - val_accuracy: 0.8799
    Epoch 2/10
    48000/48000 [==============================] - 10s 207us/step - loss: 0.3706 - accuracy: 0.8950 - val_loss: 0.3396 - val_accuracy: 0.9028
    Epoch 3/10
    48000/48000 [==============================] - 12s 242us/step - loss: 0.3099 - accuracy: 0.9095 - val_loss: 0.3000 - val_accuracy: 0.9153
    Epoch 4/10
    48000/48000 [==============================] - 12s 245us/step - loss: 0.2787 - accuracy: 0.9186 - val_loss: 0.2750 - val_accuracy: 0.9217
    Epoch 5/10
    48000/48000 [==============================] - 12s 244us/step - loss: 0.2548 - accuracy: 0.9252 - val_loss: 0.2543 - val_accuracy: 0.9284
    Epoch 6/10
    48000/48000 [==============================] - 12s 252us/step - loss: 0.2348 - accuracy: 0.9316 - val_loss: 0.2379 - val_accuracy: 0.9331
    Epoch 7/10
    48000/48000 [==============================] - 12s 251us/step - loss: 0.2173 - accuracy: 0.9372 - val_loss: 0.2233 - val_accuracy: 0.9353
    Epoch 8/10
    48000/48000 [==============================] - 13s 269us/step - loss: 0.2025 - accuracy: 0.9405 - val_loss: 0.2106 - val_accuracy: 0.9405
    Epoch 9/10
    48000/48000 [==============================] - 13s 265us/step - loss: 0.1898 - accuracy: 0.9446 - val_loss: 0.1987 - val_accuracy: 0.9421
    Epoch 10/10
    48000/48000 [==============================] - 12s 251us/step - loss: 0.1782 - accuracy: 0.9475 - val_loss: 0.1957 - val_accuracy: 0.9433



```python
# 8. ëª¨ë¸ í‰ê°€í•˜ê¸°
test_loss, test_acc = model.evaluate(X_test, y_test)
```

    10000/10000 [==============================] - 1s 83us/step



```python
# 9. ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•´ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ğŸ–¼

import numpy
for index in numpy.random.choice(len(y_test), 3, replace = False):
    predicted = model.predict(X_test[index:index + 1])[0]
    label = y_test[index]
    result_label = numpy.where(label == numpy.amax(label))
    result_predicted = numpy.where(predicted == numpy.amax(predicted))
    title = "Label value = %s  Predicted value = %s " % (result_label[0], result_predicted[0])

    fig = plt.figure(1, figsize = (3,3))
    ax1 = fig.add_axes((0,0,.8,.8))
    ax1.set_title(title)
    images = X_test
    plt.imshow(images[index].reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')
    plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_0a.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_1a.png" style="width: 50%
    ; height: 300px;">
</div>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_2a.png" style="width: 50%
    ; height: 300px;">
</div>


```python
# 10. í•™ìŠµ ì‹œê°í™”í•˜ê¸°

plt.plot(history.history['val_accuracy'])
plt.plot(history.history['accuracy'])
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_12_0a.png" style="width: 50%
    ; height: 300px;">
</div>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_12_1a.png" style="width: 50%
    ; height: 300px;">
</div>


## ì‹¤ìŠµ4
ìœ„ì—ì„œ í•™ìŠµí•œ ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Fashion MNIST ë°ì´í„°ì— ì ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ì„ ìˆ˜í–‰í•´ë³´ì•˜ë‹¤.

#### 2019.11.24. ë”¥-ëŸ¬ë‹ ê³¼ì • CNN

### ì„¸ë²ˆì§¸ ì‹¤ìŠµ. Keras ëª¨ë¸ ìƒì„±/í•™ìŠµ - Fashion MNIST : CNN
[Keras Dataset](https://keras.io/ko/datasets/#-mnist)


```python
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
from keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,
                                                 test_size=0.2,
                                                 random_state=9)

print(X_train.shape)
print(y_train.shape)
print(X_val.shape)
print(y_val.shape)
print(X_test.shape)
print(y_test.shape)
```

    (48000, 28, 28)
    (48000,)
    (12000, 28, 28)
    (12000,)
    (10000, 28, 28)
    (10000,)


```python
# 2. ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸í•˜ê¸° ğŸ–¼
import matplotlib.pyplot as plt

image = X_train[1]

plt.imshow(image, cmap = plt.cm.gray)
```


    <matplotlib.image.AxesImage at 0x2ab820aac08>



<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_3_1b.png" style="width: 50%
    ; height: 300px;">
</div>



```python
# 3-1. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : 2ì°¨ì›->3ì°¨ì› ğŸŒŸğŸŒŸğŸŒŸ
X_train = X_train.reshape(48000, 28, 28, 1)
X_val = X_val.reshape(12000, 28, 28, 1)
X_test = X_test.reshape(10000, 28, 28, 1)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)

```

    (48000, 28, 28, 1)
    (12000, 28, 28, 1)
    (10000, 28, 28, 1)



```python
# 3-2. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ : Normalzation

X_train = X_train / 255.0
X_val = X_val / 255.0
X_test = X_test / 255.0
```


```python
# 4. Label categorical (one-hot encoding)
print(y_train[:10])

from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

print(y_train[:10])
```

    [3 1 2 6 7 3 5 1 1 5]
    [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
     [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]



```python
# 5. ëª¨ë¸ ìƒì„± : CNN
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout

model = Sequential()
model.add(Conv2D(filters=32,  # í•„í„°ì˜ ê°œìˆ˜
                kernel_size=(3,3),  # í•„í„° ì‚¬ì´ì¦ˆ
                padding='same',  # padding
                activation='relu',  # activation í•¨ìˆ˜ ì„¤ì •
                input_shape=(28, 28, 1))) # (input_dim)

model.add(MaxPool2D(pool_size=(2, 2))) # ë³´í†µ (2,2)ë¥¼ ë„£ìŒ
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))# Dropout ì„¤ì •
model.add(Dense(10, activation='softmax')) # ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œì´ê¸° ë•Œë¬¸ì— softmax

print(model.summary())
```

    Model: "sequential_5"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_5 (Conv2D)            (None, 28, 28, 32)        320       
    _________________________________________________________________
    max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         
    _________________________________________________________________
    flatten_5 (Flatten)          (None, 6272)              0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               802944    
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_6 (Dense)              (None, 10)                1290      
    =================================================================
    Total params: 804,554
    Trainable params: 804,554
    Non-trainable params: 0
    _________________________________________________________________
    None



```python
# 6. Compile - Optimizer, Loss function ì„¤ì •
model.compile(loss='categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])
```


```python
# 7. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
batch_size = 128
epochs = 10

history = model.fit(X_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_val, y_val), # validation_set ì ìš© (ê¼­ ê°™ì´ í•´ì£¼ëŠ”ê²Œ ì¢‹ìŒ)
                    verbose=1)
```

    Train on 48000 samples, validate on 12000 samples
    Epoch 1/10
    48000/48000 [==============================] - 25s 511us/step - loss: 0.5189 - accuracy: 0.8164 - val_loss: 0.3606 - val_accuracy: 0.8741
    Epoch 2/10
    48000/48000 [==============================] - 23s 488us/step - loss: 0.3464 - accuracy: 0.8777 - val_loss: 0.3015 - val_accuracy: 0.8932
    Epoch 3/10
    48000/48000 [==============================] - 23s 487us/step - loss: 0.2950 - accuracy: 0.8942 - val_loss: 0.2767 - val_accuracy: 0.9023
    Epoch 4/10
    48000/48000 [==============================] - 23s 484us/step - loss: 0.2648 - accuracy: 0.9041 - val_loss: 0.2626 - val_accuracy: 0.9061
    Epoch 5/10
    48000/48000 [==============================] - 23s 480us/step - loss: 0.2444 - accuracy: 0.9106 - val_loss: 0.2510 - val_accuracy: 0.9106
    Epoch 6/10
    48000/48000 [==============================] - 23s 479us/step - loss: 0.2241 - accuracy: 0.9170 - val_loss: 0.2577 - val_accuracy: 0.9068
    Epoch 7/10
    48000/48000 [==============================] - 24s 503us/step - loss: 0.2093 - accuracy: 0.9236 - val_loss: 0.2417 - val_accuracy: 0.9126
    Epoch 8/10
    48000/48000 [==============================] - 24s 509us/step - loss: 0.1926 - accuracy: 0.9288 - val_loss: 0.2380 - val_accuracy: 0.9153
    Epoch 9/10
    48000/48000 [==============================] - 24s 504us/step - loss: 0.1821 - accuracy: 0.9325 - val_loss: 0.2352 - val_accuracy: 0.9190
    Epoch 10/10
    48000/48000 [==============================] - 24s 506us/step - loss: 0.1699 - accuracy: 0.9368 - val_loss: 0.2329 - val_accuracy: 0.9191



```python
# 8. ëª¨ë¸ í‰ê°€í•˜ê¸°
test_loss, test_acc = model.evaluate(X_test, y_test)
```

    10000/10000 [==============================] - 2s 206us/step



```python
# 9. ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•´ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ğŸ–¼
import numpy
for index in numpy.random.choice(len(y_test), 3, replace = False):
    predicted = model.predict(X_test[index:index + 1])[0]
    label = y_test[index]
    result_label = numpy.where(label == numpy.amax(label))
    result_predicted = numpy.where(predicted == numpy.amax(predicted))
    title = "Label value = %s  Predicted value = %s " % (result_label[0], result_predicted[0])

    fig = plt.figure(1, figsize = (3,3))
    ax1 = fig.add_axes((0,0,.8,.8))
    ax1.set_title(title)
    images = X_test
    plt.imshow(images[index].reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')
    plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_0b.png" style="width: 50%
    ; height: 300px;">
</div>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_1b.png" style="width: 50%
    ; height: 300px;">
</div>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_2b.png" style="width: 50%
    ; height: 300px;">
</div>



```python
# 10. í•™ìŠµ ì‹œê°í™”í•˜ê¸°

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_12_0b.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_12_1b.png" style="width: 50%
    ; height: 300px;">
</div>


## ì‹¤ìŠµ5
ë§ˆì§€ë§‰ìœ¼ë¡œ cifar10 ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´ë¯¸ì§€ ë¶„ë¥˜(Image Classification)ë¥¼ ìˆ˜í–‰í•˜ëŠ” Convolutional Neural Networks(CNNs)ì„ ë§Œë“œëŠ” ì˜ˆì œë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤..

CIFAR-10ì€ ì´ë¯¸ì§€ ì¸ì‹ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ì´ë‹¤. CIFAR-10 ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ ì´ 10ê°œì˜ ë ˆì´ë¸”ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.

airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/cifar.png" style="width: 50%
    ; height: 300px;">
</div>


ê°ê°ì˜ ë ˆì´ë¸”ë§ˆë‹¤ 32Ã—32 í¬ê¸° ì´ë¯¸ì§€ì¸ 50,000ê°œì˜ training ë°ì´í„°ì…‹, 10,000ê°œì˜ test ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ê³ , ê²°ê³¼ì ìœ¼ë¡œ ì´ 60,000ê°œì˜ 32Ã—32 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì…‹ì´ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
#### 2019.11.24. ë”¥-ëŸ¬ë‹ ê³¼ì • CNN

### ë„¤ë²ˆì§¸ ì‹¤ìŠµ. Keras ëª¨ë¸ ìƒì„±/í•™ìŠµ - cifar10 : CNN
[Keras Dataset](https://keras.io/ko/datasets/#-cifar10)


```python
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,
                                                 test_size=0.2,
                                                 random_state=9)

print(X_train.shape)
print(y_train.shape)
print(X_val.shape)
print(y_val.shape)
print(X_test.shape)
print(y_test.shape)

```

    (40000, 32, 32, 3)
    (40000, 1)
    (10000, 32, 32, 3)
    (10000, 1)
    (10000, 32, 32, 3)
    (10000, 1)



```python
# 2. ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸í•˜ê¸° ğŸ–¼
import matplotlib.pyplot as plt

image = X_train[2]

plt.imshow(image)
```

    <matplotlib.image.AxesImage at 0x2a31e4ed948>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_3_1c.png" style="width: 50%
    ; height: 300px;">
</div>


```python
# 3. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬
X_train = X_train / 255.0
X_val = X_val / 255.0
X_test = X_test / 255.0
```


```python
# 4. Label categorical (one-hot encoding)
# print(y_train[:10])
from keras.utils import to_categorical

y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

print(y_train[:10])

```

    [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
     [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
     [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
     [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]



```python
# 5. ëª¨ë¸ ìƒì„± : CNN
from keras.models import Sequential
from keras.layers import Dropout, Dense, Conv2D, MaxPooling2D, Flatten

model = Sequential()
model.add(Conv2D(filters=32,
                kernel_size=(3,3),
                padding='same',
                activation='relu',
                input_shape=(32, 32, 3)))

model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

print(model.summary())

```

    Model: "sequential_7"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_10 (Conv2D)           (None, 32, 32, 32)        896       
    _________________________________________________________________
    conv2d_11 (Conv2D)           (None, 32, 32, 64)        18496     
    _________________________________________________________________
    max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         
    _________________________________________________________________
    flatten_7 (Flatten)          (None, 16384)             0         
    _________________________________________________________________
    dense_16 (Dense)             (None, 128)               2097280   
    _________________________________________________________________
    dropout_11 (Dropout)         (None, 128)               0         
    _________________________________________________________________
    dense_17 (Dense)             (None, 64)                8256      
    _________________________________________________________________
    dropout_12 (Dropout)         (None, 64)                0         
    _________________________________________________________________
    dense_18 (Dense)             (None, 10)                650       
    =================================================================
    Total params: 2,125,578
    Trainable params: 2,125,578
    Non-trainable params: 0
    _________________________________________________________________
    None



```python
# 6. Compile - Optimizer, Loss function ì„¤ì •
model.compile(loss='categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])
```


```python
# 7. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
batch_size = 128
epochs = 10

history = model.fit(X_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_val, y_val), # validation_set ì ìš© (ê¼­ ê°™ì´ í•´ì£¼ëŠ”ê²Œ ì¢‹ìŒ)
                    verbose=1)
```

    Train on 40000 samples, validate on 10000 samples
    Epoch 1/10
    40000/40000 [==============================] - 67s 2ms/step - loss: 1.7432 - accuracy: 0.3568 - val_loss: 1.3433 - val_accuracy: 0.5201
    Epoch 2/10
    40000/40000 [==============================] - 72s 2ms/step - loss: 1.3271 - accuracy: 0.5257 - val_loss: 1.1500 - val_accuracy: 0.5906
    Epoch 3/10
    40000/40000 [==============================] - 77s 2ms/step - loss: 1.1586 - accuracy: 0.5886 - val_loss: 1.0559 - val_accuracy: 0.6271
    Epoch 4/10
    40000/40000 [==============================] - 79s 2ms/step - loss: 1.0281 - accuracy: 0.6375 - val_loss: 0.9819 - val_accuracy: 0.6544
    Epoch 5/10
    40000/40000 [==============================] - 77s 2ms/step - loss: 0.9405 - accuracy: 0.6692 - val_loss: 0.9503 - val_accuracy: 0.6616
    Epoch 6/10
    40000/40000 [==============================] - 74s 2ms/step - loss: 0.8635 - accuracy: 0.6951 - val_loss: 0.9583 - val_accuracy: 0.6658
    Epoch 7/10
    40000/40000 [==============================] - 80s 2ms/step - loss: 0.7953 - accuracy: 0.7214 - val_loss: 0.9359 - val_accuracy: 0.6790
    Epoch 8/10
    40000/40000 [==============================] - 74s 2ms/step - loss: 0.7194 - accuracy: 0.7470 - val_loss: 0.9354 - val_accuracy: 0.6783
    Epoch 9/10
    40000/40000 [==============================] - 79s 2ms/step - loss: 0.6575 - accuracy: 0.7661 - val_loss: 0.9470 - val_accuracy: 0.6804
    Epoch 10/10
    40000/40000 [==============================] - 76s 2ms/step - loss: 0.6005 - accuracy: 0.7867 - val_loss: 1.0215 - val_accuracy: 0.6707



```python
# 8. ëª¨ë¸ í‰ê°€í•˜ê¸°
test_loss, test_acc = model.evaluate(X_test, y_test)
```

    10000/10000 [==============================] - 5s 517us/step



```python
# 9. ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•´ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ğŸ–¼
import numpy
for index in numpy.random.choice(len(y_test), 3, replace = False):
    predicted = model.predict(X_test[index:index + 1])[0]
    label = y_test[index]
    result_label = numpy.where(label == numpy.amax(label))
    result_predicted = numpy.where(predicted == numpy.amax(predicted))
    title = "Label value = %s  Predicted value = %s " % (result_label[0], result_predicted[0])

    fig = plt.figure(1, figsize = (3,3))
    ax1 = fig.add_axes((0,0,.8,.8))
    ax1.set_title(title)
    images = X_test
    plt.imshow(images[index], interpolation = 'nearest')
    plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_10_0c.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_10_1c.png" style="width: 50%
    ; height: 300px;">
</div>

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_10_2c.png" style="width: 50%
    ; height: 300px;">
</div>



```python
# 10. í•™ìŠµ ì‹œê°í™”í•˜ê¸°

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_0c.png" style="width: 50%
    ; height: 300px;">
</div>


<div style="width: 100%; height: 300px;">
    <img src="https://kyu9341.github.io/assets/output_11_1c.png" style="width: 50%
    ; height: 300px;">
</div>



ì°¸ì¡°
<https://umbum.tistory.com/223>
<https://untitledtblog.tistory.com/150>
<https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/>
<http://solarisailab.com/archives/1700>
